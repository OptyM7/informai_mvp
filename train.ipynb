{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efc89947-c94e-4cce-aab9-32104adef3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1517 MiB |  21712 MiB | 449846 GiB | 449844 GiB |\n",
      "|       from large pool |   1516 MiB |  21709 MiB | 449212 GiB | 449210 GiB |\n",
      "|       from small pool |      1 MiB |      4 MiB |    634 GiB |    634 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1517 MiB |  21712 MiB | 449846 GiB | 449844 GiB |\n",
      "|       from large pool |   1516 MiB |  21709 MiB | 449212 GiB | 449210 GiB |\n",
      "|       from small pool |      1 MiB |      4 MiB |    634 GiB |    634 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1497 MiB |  21698 MiB | 448625 GiB | 448624 GiB |\n",
      "|       from large pool |   1495 MiB |  21694 MiB | 447992 GiB | 447990 GiB |\n",
      "|       from small pool |      1 MiB |      4 MiB |    633 GiB |    633 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   2164 MiB |  22196 MiB |  74796 MiB |  72632 MiB |\n",
      "|       from large pool |   2160 MiB |  22192 MiB |  74788 MiB |  72628 MiB |\n",
      "|       from small pool |      4 MiB |      6 MiB |      8 MiB |      4 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 661833 KiB |   3162 MiB | 326188 GiB | 326187 GiB |\n",
      "|       from large pool | 658841 KiB |   3160 MiB | 325554 GiB | 325553 GiB |\n",
      "|       from small pool |   2992 KiB |      3 MiB |    634 GiB |    634 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     506    |    1089    |   20233 K  |   20233 K  |\n",
      "|       from large pool |     236    |     582    |   14599 K  |   14599 K  |\n",
      "|       from small pool |     270    |     626    |    5633 K  |    5633 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     506    |    1089    |   20233 K  |   20233 K  |\n",
      "|       from large pool |     236    |     582    |   14599 K  |   14599 K  |\n",
      "|       from small pool |     270    |     626    |    5633 K  |    5633 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      36    |     122    |     404    |     368    |\n",
      "|       from large pool |      34    |     119    |     400    |     366    |\n",
      "|       from small pool |       2    |       3    |       4    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      79    |    9303 K  |    9303 K  |\n",
      "|       from large pool |      25    |      70    |    7333 K  |    7333 K  |\n",
      "|       from small pool |      11    |      20    |    1970 K  |    1970 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353add239c9e4ec3af8515e7c4dca134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b8b1e832a94d21a97a4fe1cc5b7eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6952cc4663914e39b08119b18b0146d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee5d78f98fb4f1db185b330be4da8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments, OPTForQuestionAnswering, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "# load data set\n",
    "df = pd.read_csv('trainsetfull.csv').loc[:2000] # only importing 2000 examples\n",
    "# combine question and answer into a single column field\n",
    "df['text'] = df.apply(lambda x: f\"Q: {x['question']} A: {x['answer']}\", axis=1)\n",
    "\n",
    "# get training and test data sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# convert datasets to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['text']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "# Load GPT-Neo model (GPT-2's input token limit was too small for dataset)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M')  # Choose the model size here\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation = True, padding='max_length', max_length=1150)\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for Trainer\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\"])\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\"])\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "\n",
    "# Define a function to create labels\n",
    "def create_labels(batch):\n",
    "    batch['labels'] = batch['input_ids']  # Use the input_ids as labels\n",
    "    return batch\n",
    "\n",
    "# Apply the create_labels function\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(create_labels)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(create_labels)\n",
    "\n",
    "# 1. increase epoch size\n",
    "# 2. add a padding to \n",
    "# 3. Use a pre-finetuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c4626d-e781-42b0-ac85-1639aab00eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11dec58-ece2-4424-a4dc-6edf05e4114d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 37:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.888623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.015732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.012511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.011588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.011373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.010998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.010790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.010722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.010698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.010702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8000, training_loss=0.11656168040633201, metrics={'train_runtime': 2223.8634, 'train_samples_per_second': 7.195, 'train_steps_per_second': 3.597, 'total_flos': 9387130060800000.0, 'train_loss': 0.11656168040633201, 'epoch': 10.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./result',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit = 1,\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,  # Add the eval_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5783a7ac-8675-494b-830a-2586adb9641d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('finetuned_gptneo/tokenizer_config.json',\n",
       " 'finetuned_gptneo/special_tokens_map.json',\n",
       " 'finetuned_gptneo/vocab.json',\n",
       " 'finetuned_gptneo/merges.txt',\n",
       " 'finetuned_gptneo/added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.save_model(\"finetuned_gptneo\")\n",
    "tokenizer.save_pretrained(\"finetuned_gptneo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16d9b924-34f0-41ed-a3b8-f699fa810506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 45 Oak Street, Springfield\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('finetuned_gptneo')  # Choose the model size here\n",
    "tokenizer.pad_token = 'eos_token_id'\n",
    "model = GPTNeoForCausalLM.from_pretrained('finetuned_gptneo')\n",
    "\n",
    "#Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a function to generate responses\n",
    "def generate_response(question, max_length=900):\n",
    "    # Prepare the input\n",
    "    input_text = f\"Q: {question} A:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "   # attention_mask = input_ids['attention_mask']\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer from the response\n",
    "    answer = response.split(\"A:\")[-1].strip()  # Get the text after \"A:\"\n",
    "    return answer\n",
    "\n",
    "# testing\n",
    "question = \"\"\"Q: GP: Good morning! What brings you in today?\n",
    "\n",
    "Patient: Hi, Iâ€™ve been feeling really tired and fatigued lately.\n",
    "\n",
    "GP: I see. Iâ€™m going to ask you some questions to gather more information. Can I start with your Medicare card number?\n",
    "\n",
    "Patient: Sure, itâ€™s 1234 5678 9311.\n",
    "\n",
    "GP: Great. And your last name?\n",
    "\n",
    "Patient: Smith.\n",
    "\n",
    "GP: Thank you, Ms. Smith. And your given name?\n",
    "\n",
    "Patient: Itâ€™s Emily.\n",
    "\n",
    "GP: And your date of birth?\n",
    "\n",
    "Patient: Itâ€™s March 12, 1985.\n",
    "\n",
    "GP: Emily, thank you. What is your sex?\n",
    "\n",
    "Patient: Female.\n",
    "\n",
    "GP: Perfect. Now, could you provide me with your address?\n",
    "\n",
    "Patient: I live at 45 Oak Street, Springfield.\n",
    "\n",
    "GP: Thanks. And whatâ€™s your home telephone number?\n",
    "\n",
    "Patient: Itâ€™s 02 1234 5678.\n",
    "\n",
    "GP: Great. Do you have a work phone number?\n",
    "\n",
    "Patient: Yes, itâ€™s 02 8765 4321.\n",
    "\n",
    "GP: And your mobile number?\n",
    "\n",
    "Patient: Itâ€™s 0400 123 056.\n",
    "\n",
    "GP: Are you currently fasting?\n",
    "\n",
    "Patient: No, Iâ€™m not fasting.\n",
    "\n",
    "GP: Do you have a pension card, healthcare concession card, or a veteran affairs card?\n",
    "\n",
    "Patient: I have a veteran affairs card.\n",
    "\n",
    "GP: Could you please provide me with your veteran affairs card number?\n",
    "\n",
    "Patient: Itâ€™s VA123456.\n",
    "\n",
    "GP: Thank you. You mentioned you donâ€™t have a pension card or a Repat Gold card, correct?\n",
    "\n",
    "Patient: Thatâ€™s right.\n",
    "\n",
    "GP: Thank you for the information. Now, for billing, do you prefer private, concession, or bulk billing?\n",
    "\n",
    "Patient: Iâ€™d prefer bulk billing, please.\n",
    "\n",
    "GP: Alright, Emily. Based on your symptoms, Iâ€™ll recommend some blood tests to check for any underlying issues. Letâ€™s see what we can find out.\n",
    "\n",
    "GPâ€™s Notes:\n",
    "Emily Smith, born on March 12, 1985, resides at 45 Oak Street, Springfield. She is a female patient, with a Medicare card number of 1234 5678 9311 and a veteran affairs card number of VA123456. Her contact numbers include a home phone at 02 1234 5678, a work phone at 02 8765 4321, and a mobile number of 0400 123 456. The patient is not fasting and prefers bulk billing for her tests. The results will be collected by the patient. I will fax the bloodwork results to me upon completion, and my doctor number is 987654, with the surname Johnson and initials J.D. The report will be sent to my office at 20 Clinic Road, Springfield. The tests are not marked as urgent.\n",
    "\n",
    "\n",
    "*What is the patient's address*\\\" A:\"\"\"\n",
    "response = generate_response(question)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ce3e1-141e-4ad8-8cdc-d16a17e5dd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16369f-502c-463d-bb84-05509c5ec4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
